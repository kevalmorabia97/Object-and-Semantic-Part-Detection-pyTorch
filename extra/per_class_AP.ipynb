{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from datasets import load_data\n",
    "from utils import compute_per_class_AP\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import JointDetector\n",
    "from train_joint import evaluate\n",
    "\n",
    "\n",
    "# For Joint Model\n",
    "_, val_loader, obj_class2ind, obj_n_classes, part_class2ind, part_n_classes = load_data('../data/VOCdevkit/VOC2010/', 1, 'animals_train', 'animals_val',\n",
    "    'animals_object_class2ind', True, True, True, 'animals_part_mergedclass2ind', 0, None)\n",
    "\n",
    "obj_ind2class = {v: k for k,v in obj_class2ind.items()}\n",
    "part_ind2class = {v: k for k,v in part_class2ind.items()}\n",
    "\n",
    "model = JointDetector(obj_n_classes, part_n_classes, 0.9, False).to(device)\n",
    "model.load_state_dict(torch.load('../saved_model_joint_ft-0.90_animals_train.pth', map_location=device)['model'])\n",
    "\n",
    "obj_coco_evaluator, part_coco_evaluator, _, _ = evaluate(model, val_loader, device, 500)\n",
    "\n",
    "print('\\nObject classwise AP@IoU=0.5:')\n",
    "obj_per_class_AP = compute_per_class_AP(obj_coco_evaluator)\n",
    "for c in range(1, obj_n_classes):\n",
    "    print('%-5s: %.2f' % (obj_ind2class[c], 100*obj_per_class_AP[c-1]))\n",
    "\n",
    "print('\\nPart classwise AP@IoU=0.5:')\n",
    "part_per_class_AP = compute_per_class_AP(part_coco_evaluator)\n",
    "for c in range(1, obj_n_classes):\n",
    "    print('%-5s: %.2f' % (part_ind2class[c], 100*part_per_class_AP[c-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import get_FasterRCNN_model\n",
    "from references.detection.engine import evaluate\n",
    "\n",
    "\n",
    "##### For FasterRCNN Object Detection Model #####\n",
    "_, val_loader, class2ind, n_classes = load_data('../data/VOCdevkit/VOC2010/', 1, 'animals_train', 'animals_val',\n",
    "    'animals_object_class2ind', True, False, False, None, 0, None)\n",
    "model_save_path = '../saved_model_object_animals_train_mAP.pth'\n",
    "\n",
    "##### For FasterRCNN Part Detection Model #####\n",
    "# _, val_loader, class2ind, n_classes = load_data('../data/VOCdevkit/VOC2010/', 1, 'animals_train', 'animals_val',\n",
    "#     'animals_part_mergedclass2ind', False, True, False, None, 0, None)\n",
    "# model_save_path = '../saved_model_part_animals_train.pth'\n",
    "\n",
    "\n",
    "ind2class = {v: k for k,v in class2ind.items()}\n",
    "\n",
    "model = get_FasterRCNN_model(n_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device)['model'])\n",
    "\n",
    "coco_evaluator, _ = evaluate(model, val_loader, device, 500)\n",
    "\n",
    "print('\\nClasswise AP@IoU=0.5:')\n",
    "per_class_AP = compute_per_class_AP(coco_evaluator)\n",
    "for c in range(1, n_classes):\n",
    "    print('%-5s: %.2f' % (ind2class[c], 100*per_class_AP[c-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
